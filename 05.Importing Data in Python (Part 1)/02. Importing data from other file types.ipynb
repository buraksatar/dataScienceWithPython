{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've learned how to import flat files, but there are many other file types you will potentially have to work with as a data scientist. In this chapter, you'll learn how to import data into Python from a wide array of important file types. You will be importing file types such as pickled files, Excel spreadsheets, SAS and Stata files, HDF5 files, a file type for storing large quantities of numerical data, and MATLAB files.\n",
    "\n",
    "# 1. Introduction to other file types\n",
    "\n",
    "Pickled files:\n",
    "- File type native to Python\n",
    "- Motivation: many datatypes for which it isn't obvious how to store them\n",
    "- Pickled files are serialized\n",
    "- Serialize = conver object to bytestream\n",
    "\n",
    "### Not so flat any more\n",
    "\n",
    "In Chapter 1, you learned how to use the IPython magic command `! ls` to explore your current working directory. You can also do this natively in Python using the [library os](https://docs.python.org/2/library/os.html), which consists of miscellaneous operating system interfaces.\n",
    "\n",
    "The first line of the following code imports the library 'os', the second line stores the name of the current directory in a string called 'wd' and the third outputs the contents of the directory in a list to the shell.\n",
    "\n",
    "```\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "os.listdir(wd)\n",
    "```\n",
    "\n",
    "file.xlsx is not a flat because it is a spreadsheet consisting of many sheets, not a single table.\n",
    "\n",
    "### Loading a pickled file\n",
    "\n",
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can `serialize` them. All this means is converting the object into a sequence of bytes, or a bytestream.\n",
    "\n",
    "In this exercise, you'll import the `pickle` package, open a previously pickled data structure from a file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "# Complete the second argument of open() so that it is read only for a binary file. This argument will be a string of two letters, one signifying 'read only', the other 'binary'.\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    # Pass the correct argument to pickle.load(); it should use the variable that is bound to open.\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing sheets in Excel files\n",
    "\n",
    "Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in time. You won't always want to do so in Excel, however!\n",
    "\n",
    "Here, you'll learn how to use `pandas` to import Excel spreadsheets and how to list the names of the sheets in any loaded .xlsx file.\n",
    "\n",
    "Recall from the video that, given an Excel file imported into a variable `spreadsheet`, you can retrieve a list of the sheet names using the attribute `spreadsheet.sheet_names`.\n",
    "\n",
    "Specifically, you'll be loading and checking out the spreadsheet `'battledeath.xlsx'`, modified from the Peace Research Institute Oslo's (PRIO) [dataset](https://www.prio.org/Data/Armed-Conflict/Battle-Deaths/The-Battle-Deaths-Dataset-version-30/). This data contains age-adjusted mortality rates due to war in various countries over several years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xl\n",
    "x1 = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(x1.sheet_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing sheets from Excel files\n",
    "\n",
    "In the previous exercises, you saw that the Excel file contains two sheets, `'2002'` and `'2004'`. The next step is to import these.\n",
    "\n",
    "In this exercise, you'll learn how to import any given sheet of your loaded .xlsx file as a DataFrame. You'll be able to do so by specifying either the sheet's name or its index.\n",
    "\n",
    "The spreadsheet `'battledeath.xlsx'` is already loaded as `x1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xl.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2 = xl.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll typically find yourself referring to the Excel sheet by name, but it's good to know you can also use indexes.\n",
    "\n",
    "### Customizing your spreadsheet import\n",
    "\n",
    "Here, you'll parse your spreadsheets and use additional arguments to skip rows, rename columns and select only particular columns.\n",
    "\n",
    "The spreadsheet `'battledeath.xlsx'` is already loaded as `xl`.\n",
    "\n",
    "As before, you'll use the method `parse()`. This time, however, you'll add the additional arguments `skiprows`, `names` and `parse_cols`. These skip rows, name the columns and designate which columns to parse, respectively. All these arguments can be assigned to lists containing the specific row numbers, strings and column numbers, as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xl.parse(0, skiprows=[1], names=['Country', 'AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xl.parse(1, parse_cols=[0], skiprows=1, names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing SAS/Stata files using pandas\n",
    "\n",
    "### Importing SAS files\n",
    "\n",
    "In this exercise, you'll figure out how to import a SAS file as a DataFrame using `SAS7BDAT` and `pandas`. The file `'sales.sas7bdat'` is already in your working directory and both `pandas` and `matplotlib.pyplot` have already been imported as follows:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "The data are adapted from the website of the undergraduate text book [Principles of Econometrics](http://www.principlesofeconometrics.com/sas/) by Hill, Griffiths and Lim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sas7bdat package\n",
    "# Import the module SAS7BDAT from the library sas7bdat\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "# In the context of the file 'sales.sas7bdat', load its contents to a DataFrame df_sas, using the method to_data_frame() on the object file\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using read_stata to import Stata files\n",
    "\n",
    "The `pandas` package has been imported in the environment as `pd` and the file `disarea.dta` is in your working directory. The data consist of disease extents for several diseases in various countries (more information can be found [here](https://www.hks.harvard.edu/centers/cid)).\n",
    "\n",
    "Here, you'll gain expertise in importing Stata files as DataFrames using the `pd.read_stata()` function from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df = pd.read_stata('disarea.dta')\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Importing HDF5 files\n",
    "\n",
    "- It is standard for storing large quantities of numerical data (giga, tera, exa).\n",
    "\n",
    "The `h5py` package has been imported in the environment and the file `LIGO_data.hdf5` is loaded in the object `h5py_file`.\n",
    "\n",
    "### Using h5py to import HDF5 files\n",
    "\n",
    "In this exercise, you'll import it using the `h5py` library. You'll also print out its datatype to confirm you have imported it correctly. You'll then study the structure of the file in order to see precisely what HDF groups it contains.\n",
    "\n",
    "You can find the LIGO data plus loads of documentation and tutorials [here](https://losc.ligo.org/events/GW150914/). There is also a great tutorial on Signal Processing with the data [here](https://losc.ligo.org/s/events/GW150914/GW150914_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename: file\n",
    "file = 'LIGO_data.hdf5'\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))\n",
    "\n",
    "# Print the keys of the file\n",
    "# Print the names of the groups in the HDF5 file 'LIGO_data.hdf5'.\n",
    "for key in data.keys():\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from your HDF5 file\n",
    "\n",
    "In this exercise, you'll extract some of the LIGO experiment's actual data from the HDF5 file and you'll visualize it.\n",
    "\n",
    "To do so, you'll need to first explore the HDF5 group `'strain'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HDF5 group: group\n",
    "group = data['strain']\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain = data['strain']['Strain'].value\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples = 10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "# produce a plot of the time series data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing MATLAB files\n",
    "\n",
    "- keys = MATLAB variable names\n",
    "- values = objects assigned to variables\n",
    "\n",
    "\n",
    "### Loading .mat files\n",
    "\n",
    "In this exercise, you'll figure out how to load a MATLAB file using `scipy.io.loadmat()` and you'll discover what Python datatype it yields.\n",
    "\n",
    "The file `'albeck_gene_expression.mat'` is in your working directory. This file contains [gene expression data](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm) from the Albeck Lab at UC Davis. You can find the data and some great documentation [here](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat = scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(type(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure of .mat in Python\n",
    "\n",
    "Here, you'll discover what is in the MATLAB dictionary that you loaded in the previous exercise.\n",
    "\n",
    "The file `'albeck_gene_expression.mat'` is already loaded into the variable `mat`. The following libraries have already been imported as follows:\n",
    "\n",
    "```\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "Once again, this file contains [gene expression data](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm) from the Albeck Lab at UCDavis. You can find the data and some great documentation [here](https://www.mcb.ucdavis.edu/faculty-labs/albeck/workshop.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the keys of the MATLAB dictionary\n",
    "# Use the method .keys() on the dictionary mat to print the keys. Most of these keys (in fact the ones that do NOT begin and end with '__') are variables from the corresponding MATLAB environment.\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(np.shape(mat['CYratioCyt']))\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
